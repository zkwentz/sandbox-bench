# sandbox-bench ðŸŽï¸

Open source benchmark suite for AI agent sandbox providers.

**Measure what matters:** Time, cost, errors, and friction when an AI agent onboards to your sandbox.

## Why?

AI agents are the new power users. They don't read your UI â€” they parse your docs, call your API, and move on. This benchmark measures how well sandbox providers serve AI agents.

## Providers Tested

| Provider | Type | Status |
|----------|------|--------|
| [E2B](https://e2b.dev) | Firecracker microVM | âœ… Supported |
| [Daytona](https://daytona.io) | Docker | âœ… Supported |
| [Modal](https://modal.com) | Container | âœ… Supported |
| [CodeSandbox](https://codesandbox.io) | Docker | âœ… Supported |
| [Fly.io Machines](https://fly.io) | Firecracker | âœ… Supported |
| [Freestyle](https://freestyle.sh) | Container | âœ… Supported |
| [Blaxel](https://blaxel.ai) | Container | âœ… Supported |
| Custom | Any | âœ… Pluggable |

## Metrics

| Metric | Description | Weight |
|--------|-------------|--------|
| **Time** | Seconds from API key to working sandbox | 30% |
| **Tool Calls** | Number of API/SDK calls required | 15% |
| **Friction** | Manual steps or workarounds needed | 15% |
| **Errors** | Errors encountered during onboarding | 20% |
| **Cost** | USD cost per benchmark run | 10% |
| **Discoverability** | How easy to find correct API usage | 10% |

## Quick Start

```bash
# Install
pip install sandbox-bench

# Run benchmark against all providers (needs API keys in env)
sandbox-bench run --all

# Run against specific provider
sandbox-bench run --provider e2b

# Run with specific model
sandbox-bench run --all --model claude-opus-4

# Output JSON results
sandbox-bench run --all --output results.json
```

## Configuration

Set API keys via environment variables:

```bash
export E2B_API_KEY="..."
export DAYTONA_API_KEY="..."
export MODAL_TOKEN_ID="..."
export MODAL_TOKEN_SECRET="..."
export CODESANDBOX_API_KEY="..."
export FLY_API_TOKEN="..."
export ANTHROPIC_API_KEY="..."  # For the benchmark agent
```

Or use a `.env` file:

```bash
sandbox-bench run --all --env-file .env
```

## The Benchmark Task

The benchmark measures how quickly an AI agent can:

1. **Authenticate** â€” Use the API key to connect
2. **Create sandbox** â€” Spin up a new isolated environment
3. **Execute code** â€” Run a simple Python script
4. **Read output** â€” Capture stdout/stderr
5. **File I/O** â€” Write and read a file
6. **Cleanup** â€” Destroy the sandbox

This represents a minimal "hello world" for sandbox providers â€” the baseline any AI coding agent needs.

## Sample Output

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    sandbox-bench results                         â”‚
â”‚                    Model: claude-opus-4                          â”‚
â”‚                    Date: 2026-02-21                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¤
â”‚ Provider     â”‚ Time   â”‚ Calls â”‚ Friction â”‚ Errors â”‚ Cost â”‚ Gradeâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ E2B          â”‚ 43s    â”‚ 13    â”‚ 1        â”‚ 0      â”‚ $0.47â”‚ A    â”‚
â”‚ Modal        â”‚ 52s    â”‚ 15    â”‚ 1        â”‚ 0      â”‚ $0.38â”‚ A    â”‚
â”‚ Daytona      â”‚ 2m 8s  â”‚ 19    â”‚ 1        â”‚ 1      â”‚ $0.52â”‚ B    â”‚
â”‚ Fly.io       â”‚ 2m 45s â”‚ 22    â”‚ 2        â”‚ 1      â”‚ $0.61â”‚ B    â”‚
â”‚ CodeSandbox  â”‚ 3m 25s â”‚ 32    â”‚ 2        â”‚ 1      â”‚ $2.11â”‚ C    â”‚
â”‚ Blaxel       â”‚ 3m 46s â”‚ 34    â”‚ 1        â”‚ 1      â”‚ $1.01â”‚ C    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
```

## Adding a Provider

Implement the `SandboxProvider` interface:

```python
from sandbox_bench import SandboxProvider, BenchmarkResult

class MyProvider(SandboxProvider):
    name = "my-provider"
    
    async def authenticate(self, api_key: str) -> None:
        """Connect to the provider."""
        ...
    
    async def create_sandbox(self) -> str:
        """Create a new sandbox, return its ID."""
        ...
    
    async def execute(self, sandbox_id: str, code: str) -> tuple[str, str]:
        """Execute code, return (stdout, stderr)."""
        ...
    
    async def write_file(self, sandbox_id: str, path: str, content: str) -> None:
        """Write a file to the sandbox."""
        ...
    
    async def read_file(self, sandbox_id: str, path: str) -> str:
        """Read a file from the sandbox."""
        ...
    
    async def destroy(self, sandbox_id: str) -> None:
        """Destroy the sandbox."""
        ...

# Register it
from sandbox_bench import register_provider
register_provider(MyProvider)
```

## How Scoring Works

### Grade Calculation

```
Score = (
    (1 - time_normalized) * 0.30 +
    (1 - calls_normalized) * 0.15 +
    (1 - friction_normalized) * 0.15 +
    (1 - errors_normalized) * 0.20 +
    (1 - cost_normalized) * 0.10 +
    discoverability * 0.10
) * 100

Grade:
  A  = 85-100
  B  = 70-84
  C  = 55-69
  D  = 40-54
  F  = 0-39
```

### Discoverability Score

Rated 1-5 based on:
- **5/5**: MCP server, OpenAPI spec, or llms.txt
- **4/5**: Well-structured docs with examples
- **3/5**: Docs exist but scattered or incomplete
- **2/5**: Minimal docs, mostly code comments
- **1/5**: No docs, reverse-engineer required

## Agent Mode

The benchmark can run in "agent mode" where an actual AI agent (Claude, GPT-4, etc.) attempts to use each provider from scratch:

```bash
# Let Claude figure out each SDK from docs alone
sandbox-bench run --all --agent-mode --model claude-opus-4

# Compare how different models perform
sandbox-bench run --provider e2b --agent-mode --model gpt-4o
sandbox-bench run --provider e2b --agent-mode --model claude-opus-4
```

This measures real-world agent experience, not just API performance.

## CI Integration

```yaml
# .github/workflows/benchmark.yml
name: Sandbox Benchmark
on:
  schedule:
    - cron: '0 0 * * 0'  # Weekly
  workflow_dispatch:

jobs:
  benchmark:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install sandbox-bench
      - run: sandbox-bench run --all --output results.json
        env:
          E2B_API_KEY: ${{ secrets.E2B_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      - uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: results.json
```

## Contributing

PRs welcome! Especially for:
- New provider implementations
- Improved scoring algorithms
- Better agent prompts
- Dashboard/visualization

## License

MIT

## Acknowledgments

Inspired by [2027.dev/arena](https://2027.dev/arena) â€” we wanted an open source version anyone can run and extend.
